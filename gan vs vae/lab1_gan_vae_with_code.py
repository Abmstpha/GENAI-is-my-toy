# -*- coding: utf-8 -*-
"""Lab1_GAN_VAE_with_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dACZP3Nq5OnjNfA5q2s5QFVAY2003zON

# üß™ Lab 1 ‚Äî Generative Models Foundations: **GAN vs VAE** (PyTorch, MNIST)

##BY: Abdellahi El Moustapha

**Course:** Generative AI (Day 1)  
**Lab Length:** ~3 hours  
**Goal:** Implement a **GAN** and a **VAE** on MNISTFashion-MNIST, compare their behavior, and reflect on stability vs. latent structure.

> ‚úÖ **What you will submit:**  
> - A short reflection with generated image grids and comments.

### üö¶ Rules
- Keep training epochs small if you‚Äôre on CPU; you can always re-run with more epochs later.
- Cells marked **(Provided)** can be run as-is; cells marked **(TODO)** require your edits.

### üß∞ Requirements
- PyTorch, Torchvision, Matplotlib, TQDM
- (Optional) SciPy for a simple FID-like metric

## üéØ Learning Objectives
- Implement a **vanilla GAN**: Generator + Discriminator, adversarial loss, and training loop.
- Implement a **Variational Autoencoder (VAE)**: encoder/decoder, **reparameterization trick**, and **ELBO**.
- Produce **visualizations**: sample grids, reconstructions, and **latent interpolations**.
- Compare GAN vs VAE using a **proxy FID-like** feature distance.
- Reflect on stability, mode collapse, and smoothness of latent space.
"""

from google.colab import drive
import os

# Mount Google Drive
drive.mount('/content/drive')

# Define the path for the new folder within Google Drive
GAN_PICS_DIR = '/content/drive/MyDrive/GAN_PICS'

# Create the folder if it doesn't exist
os.makedirs(GAN_PICS_DIR, exist_ok=True)
print(f"Google Drive mounted and folder '{GAN_PICS_DIR}' ensured.")

"""
## üß≠ Tips for Success
- Use **[-1, 1]** input range for GANs (Tanh output).  
- Start with **small networks**. You can always scale up.  
- Inspect **loss curves**, generated images, and reconstructions **frequently**.
- If your GAN collapses, try: smaller LR, label smoothing, or tweak BatchNorm.
"""

# ===== (Provided) Setup & Installs =====
# If you're in Colab, uncomment the next line to ensure dependencies:
# !pip -q install torch torchvision torchaudio matplotlib tqdm scipy

import math, os, random, numpy as np, torch, torch.nn as nn, torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import datasets, transforms, utils
import matplotlib.pyplot as plt
from tqdm import tqdm

SEED = 42
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("Device:", device)


def show_grid(tensor, title="", nrow=4, value_range=(-1,1), save_path=None):
    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, value_range=value_range)
    plt.figure(figsize=(4,4)); plt.axis('off'); plt.title(title)
    plt.imshow(grid.permute(1,2,0).cpu()) # Ensure cpu for plotting
    if save_path:
        plt.savefig(save_path, bbox_inches='tight', pad_inches=0.1)
        print(f"Image saved to {save_path}")
    plt.show()

def interpolate(a, b, steps=8):
    # Linear interpolation in latent space between two vectors a, b
    alphas = torch.linspace(0, 1, steps, device=a.device).view(-1,1)
    return (1-alphas)*a + alphas*b


# ===== (TODO) Data: MNIST or Fashion-MNIST =====
# Hints:
# - Normalize to mean=0.5, std=0.5 to map inputs to [-1, 1] for GAN (Tanh output)
# - Use batch size around 128 if you have a GPU, smaller if on CPU

BATCH = 128  # TODO: adjust if needed
use_fashion = True  # TODO: set to True to try Fashion-MNIST

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

if use_fashion:
    train_ds = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)
    test_ds  = datasets.FashionMNIST(root='./data',  train=False, download=True, transform=transform)
else:
    train_ds = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
    test_ds  = datasets.MNIST(root='./data',  train=False, download=True, transform=transform)

train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True, num_workers=0, pin_memory=True)
test_loader  = DataLoader(test_ds,  batch_size=BATCH, shuffle=False, num_workers=0, pin_memory=True)

# Quick sanity-check visualization
xb, yb = next(iter(train_loader))
show_grid(xb[:16], title="Real samples (normalized to [-1,1])", save_path=os.path.join(GAN_PICS_DIR, "real_samples.png"))

# ===== (Provided) Helper: FID Metric =====
# Requires scipy for sqrtm
try:
    from scipy import linalg
    SCIPY_OK = True
except Exception:
    SCIPY_OK = False
    print("SciPy not available ‚Äî skipping proxy FID. You can !pip install scipy and re-run.")

def get_features(disc, loader, n_batches=50, use_fake=False, generator=None, z_dim=64):
    disc.eval()
    feats = []
    with torch.no_grad():
        for i, (x, _) in enumerate(loader):
            if i >= n_batches: break
            x = x.to(device)
            if use_fake:
                z = torch.randn(x.size(0), z_dim, device=device)
                x = generator(z)
            _, f = disc(x)
            f = F.adaptive_avg_pool2d(f, 1).flatten(1)  # (B, C)
            feats.append(f.cpu())
    return torch.cat(feats, dim=0).numpy()

def gaussian_stats(X):
    mu = X.mean(axis=0)
    sigma = np.cov(X, rowvar=False)
    return mu, sigma

def frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):
    diff = mu1 - mu2
    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)
    if not np.isfinite(covmean).all():
        covmean = linalg.sqrtm((sigma1 + np.eye(sigma1.shape[0])*eps).dot(sigma2 + np.eye(sigma2.shape[0])*eps))
    if np.iscomplexobj(covmean):
        covmean = covmean.real
    fid = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2*covmean)
    return float(fid)

# Initialize results storage
gan_results = []
vae_results = []

"""###  Unified epochs for GAN and VAE"""

GLOBAL_EPOCHS = 10

"""
---

# Part 1 ‚Äî **GAN** (Vanilla)  *(~90 minutes)*

We will implement a simple GAN (DCGAN-ish) with:
- **Generator**: maps `z ~ N(0, I)` to image `xÃÇ`
- **Discriminator**: scores real vs fake
- **Loss**: Hinge (recommended) **or** BCE (your choice)

> **Milestones**
> 1) Implement **Generator** & **Discriminator**  
> 2) Choose **loss** (hinge recommended), set **optimizers**  
> 3) Implement **training loop** and generate sample grids every epoch
"""

# ===== (TODO) GAN Architectures =====
# Hints:
# - Use Tanh output for G (inputs are normalized to [-1, 1])
# - Use LeakyReLU in D; consider BatchNorm in G (not always in D)
# - Start small: upsample from (z_dim) -> (128*7*7) -> ConvTranspose to 14x14 -> 28x28
# - Keep IMG_CH = 1 for MNIST

# Z_DIM will be defined in the training loop
IMG_CH = 1
IMG_H  = 28
IMG_W  = 28


class Generator(nn.Module):
    def __init__(self, z_dim, img_ch=IMG_CH):
        super().__init__()
        # TODO: implement a small generator.
        # Suggested skeleton:
        # - Linear(z_dim -> 128*7*7) + BN + ReLU
        # - Unflatten to (128, 7, 7)
        # - ConvTranspose2d(128 -> 64, kernel=4, stride=2, padding=1) + BN + ReLU  # (64, 14, 14)
        # - ConvTranspose2d(64 -> 32, kernel=4, stride=2, padding=1) + BN + ReLU   # (32, 28, 28)
        # - Conv2d(32 -> 1, kernel=3, stride=1, padding=1) -> Tanh
        self.net = nn.Sequential(
            nn.Linear(z_dim, 128*7*7),
            nn.BatchNorm1d(128*7*7),
            nn.ReLU(True),
            nn.Unflatten(1, (128, 7, 7)),
            nn.ConvTranspose2d(128, 64, 4, 2, 1),
            nn.BatchNorm2d(64),
            nn.ReLU(True),
            nn.ConvTranspose2d(64, 32, 4, 2, 1),
            nn.BatchNorm2d(32),
            nn.ReLU(True),
            nn.Conv2d(32, img_ch, 3, 1, 1),
            nn.Tanh()
        )
    def forward(self, z):
        return self.net(z)

class Discriminator(nn.Module):
    def __init__(self, img_ch=IMG_CH):
        super().__init__()
        # TODO: implement a small discriminator.
        # Suggested skeleton:
        # - Conv2d(1 -> 32, 4, 2, 1) + LeakyReLU
        # - Conv2d(32 -> 64, 4, 2, 1) + BN + LeakyReLU
        # - Conv2d(64 -> 128, 3, 2, 1) + LeakyReLU
        # - Flatten -> Linear(128*4*4 -> 1)
        self.features = nn.Sequential(
            nn.Conv2d(img_ch, 32, 4, 2, 1),
            nn.LeakyReLU(0.2, True),
            nn.Conv2d(32, 64, 4, 2, 1),
            nn.BatchNorm2d(64),
            nn.LeakyReLU(0.2, True),
            nn.Conv2d(64, 128, 3, 2, 1),
            nn.LeakyReLU(0.2, True),
        )
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(128*4*4, 1)
        )
    def forward(self, x):
        f = self.features(x)
        logits = self.classifier(f).squeeze(1)
        return logits, f

G = Generator(z_dim=64).to(device)
D = Discriminator().to(device)

# Quick shape tests
with torch.no_grad():
    z = torch.randn(2, 64, device=device)
    x_fake = G(z)
    logit, f = D(x_fake)
    assert x_fake.shape == (2, 1, 28, 28), f"Got {x_fake.shape}"
    assert logit.shape[0] == 2, f"Got {logit.shape}"
print("‚úì GAN shapes look OK.")

# ===== (TODO) GAN Losses =====
# Option A: Hinge loss (recommended)
def d_loss_hinge(real_logits, fake_logits):
    # TODO: implement hinge: E[max(0, 1 - D(real))] + E[max(0, 1 + D(fake))]
    return F.relu(1.0 - real_logits).mean() + F.relu(1.0 + fake_logits).mean()

def g_loss_hinge(fake_logits):
    # TODO: implement generator hinge: -E[D(fake)]
    return -fake_logits.mean()

# Option B: BCE
# TODO: To be tested
# bce = nn.BCEWithLogitsLoss()
# def d_loss_bce(real_logits, fake_logits):
#     real_t = torch.ones_like(real_logits)
#     fake_t = torch.zeros_like(fake_logits)
#     return bce(real_logits, real_t) + bce(fake_logits, fake_t)
# def g_loss_bce(fake_logits):
#     real_t = torch.ones_like(fake_logits)
#     return bce(fake_logits, real_t)

# ===== (TODO) GAN Training =====
# Hints:
# - Alternate D then G updates
# - Sample fresh z for each update
# - Visualize fixed z grid per epoch
LR = 2e-4
betas = (0.5, 0.999)

Z_DIM_OPTIONS = [32, 64, 128]

for current_z_dim in Z_DIM_OPTIONS:
    Z_DIM = current_z_dim
    print(f"\n--- Running experiments for Z_DIM = {Z_DIM} ---")

    # Re-instantiate GAN models with current Z_DIM
    G = Generator(z_dim=Z_DIM).to(device)
    D = Discriminator().to(device)
    opt_G = torch.optim.Adam(G.parameters(), lr=LR, betas=betas)
    opt_D = torch.optim.Adam(D.parameters(), lr=LR, betas=betas)

    # GAN Training Loop
    print("Starting GAN training...")
    for epoch in range(1, GLOBAL_EPOCHS + 1):
        G.train(); D.train()
        pbar = tqdm(train_loader, desc=f"[GAN, Z_DIM={Z_DIM}] Epoch {epoch}/{GLOBAL_EPOCHS}")
        for x, _ in pbar:
            x = x.to(device)

            # (1) Update D
            z = torch.randn(x.size(0), Z_DIM, device=device)
            with torch.no_grad():
                x_fake = G(z)
            real_logits, _ = D(x)
            fake_logits, _ = D(x_fake)
            loss_D = d_loss_hinge(real_logits, fake_logits)
            opt_D.zero_grad(set_to_none=True)
            loss_D.backward()
            opt_D.step()

            # (2) Update G
            z = torch.randn(x.size(0), Z_DIM, device=device)
            x_fake = G(z)
            fake_logits, _ = D(x_fake)
            loss_G = g_loss_hinge(fake_logits)
            opt_G.zero_grad(set_to_none=True)
            loss_G.backward()
            opt_G.step()

            pbar.set_postfix({'D': f"{loss_D.item():.3f}", 'G': f"{loss_G.item():.3f}"})

    # Save GAN samples for current Z_DIM
    with torch.no_grad():
        fixed_z = torch.randn(16, Z_DIM, device=device)
        samples = G(fixed_z).cpu()
    show_grid(samples, title=f"GAN Z_DIM={Z_DIM} samples (Epochs={GLOBAL_EPOCHS})",
              save_path=os.path.join(GAN_PICS_DIR, f"gan_samples_zdim_{Z_DIM}_epochs_{GLOBAL_EPOCHS}.png"))

    # Calculate FID for GAN
    if SCIPY_OK:
        real_feats = get_features(D, test_loader, n_batches=80, use_fake=False)
        fake_feats_gan = get_features(D, test_loader, n_batches=80, use_fake=True, generator=G, z_dim=Z_DIM)
        mu_r, sig_r = gaussian_stats(real_feats)
        mu_g, sig_g = gaussian_stats(fake_feats_gan)
        fid_gan = frechet_distance(mu_r, sig_r, mu_g, sig_g)
        print(f"Proxy FID (GAN, Z_DIM={Z_DIM} vs real): {fid_gan:.2f}")
    else:
        fid_gan = float('nan')
    
    gan_results.append({'z_dim': Z_DIM, 'fid': fid_gan})


"""
---

# Part 2 ‚Äî **VAE**  *(~60 minutes)*

We will implement:
- Encoder that outputs **Œº** and **log œÉ¬≤**
- **Reparameterization trick**: `z = Œº + œÉ ‚äô Œµ`
- Decoder that reconstructs `xÃÇ`
- **ELBO** loss = reconstruction + KL divergence

> **Milestones**
> 1) Build **VAE module** (encode/reparameterize/decode)  
> 2) Implement **loss** (reconstruction + KL)  
> 3) Train and visualize reconstructions & random samples  
> 4) Do a **latent interpolation** between two test images
"""

# ===== (TODO) VAE Architecture =====
# LATENT will be defined in the training loop

class VAE(nn.Module):
    def __init__(self, latent):
        super().__init__()
        # Encoder
        self.enc = nn.Sequential(
            nn.Conv2d(1, 32, 4, 2, 1),   # 14x14
            nn.ReLU(True),
            nn.Conv2d(32, 64, 4, 2, 1),  # 7x7
            nn.ReLU(True),
            nn.Flatten()
        )
        self.enc_fc_mu  = nn.Linear(64*7*7, latent)
        self.enc_fc_log = nn.Linear(64*7*7, latent)

        # Decoder
        self.dec_fc = nn.Linear(latent, 64*7*7)
        self.dec = nn.Sequential(
            nn.Unflatten(1, (64, 7, 7)),
            nn.ConvTranspose2d(64, 32, 4, 2, 1),  # 14x14
            nn.ReLU(True),
            nn.ConvTranspose2d(32, 16, 4, 2, 1),  # 28x28
            nn.ReLU(True),
            nn.Conv2d(16, 1, 3, 1, 1),
            nn.Tanh()
        )

    def encode(self, x):
        # TODO: return mu, logvar
        h = self.enc(x)
        mu = self.enc_fc_mu(h)
        logvar = self.enc_fc_log(h)
        return mu, logvar

    def reparameterize(self, mu, logvar):
        # TODO: implement z = mu + exp(0.5*logvar) * eps
        std = (0.5*logvar).exp()
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z):
        # TODO: decode to image in [-1,1]
        h = self.dec_fc(z)
        x = self.dec(h)
        return x

    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        xhat = self.decode(z)
        return xhat, mu, logvar

vae = VAE(latent=16).to(device)

# Quick shape tests
with torch.no_grad():
    x = xb[:2].to(device)
    xhat, mu, logvar = vae(x)
    assert xhat.shape == x.shape, f"{xhat.shape} vs {x.shape}"
    assert mu.shape[-1] == 16 and logvar.shape[-1] == 16
print("‚úì VAE shapes look OK.")

# ===== (TODO) VAE Loss & Training =====
# Hint: ELBO ‚âà recon_loss + KL(q(z|x) || p(z)), with p(z)=N(0,I)
# - Use L1 or BCE for reconstruction (L1 often looks nicer on MNIST)
# - KL term: -0.5 * sum(1 + logvar - mu^2 - exp(logvar))

# ===== (TODO) VAE Loss & Training =====
# Hint: ELBO ‚âà recon_loss + KL(q(z|x) || p(z)), with p(z)=N(0,I)
# - Use L1 or BCE for reconstruction (L1 often looks nicer on MNIST)
# - KL term: -0.5 * sum(1 + logvar - mu^2 - exp(logvar))

def vae_loss(xhat, x, mu, logvar):
    recon = F.l1_loss(xhat, x, reduction='sum') / x.size(0)  # try also BCE
    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)
    return recon + kl, recon, kl

LATENT_OPTIONS = [8, 16, 32]

for current_latent_dim in LATENT_OPTIONS:
    LATENT = current_latent_dim
    print(f"\n--- Running experiments for LATENT = {LATENT} ---")

    # Re-instantiate VAE model with current LATENT
    vae = VAE(latent=LATENT).to(device)
    opt_vae = torch.optim.Adam(vae.parameters(), lr=2e-3)

    # VAE Training Loop
    print("Starting VAE training...")
    for epoch in range(1, GLOBAL_EPOCHS + 1):
        vae.train()
        losses = []
        pbar = tqdm(train_loader, desc=f"[VAE, LATENT={LATENT}] Epoch {epoch}/{GLOBAL_EPOCHS}")
        for x, _ in pbar:
            x = x.to(device)
            xhat, mu, logvar = vae(x)
            loss, rec, kl = vae_loss(xhat, x, mu, logvar)
            opt_vae.zero_grad(set_to_none=True)
            loss.backward()
            opt_vae.step()
            losses.append(loss.item())
            pbar.set_postfix({'loss': f"{np.mean(losses):.2f}"})

    # Visualize VAE outputs for current LATENT
    vae.eval()
    with torch.no_grad():
        x_test_batch, _ = next(iter(test_loader))
        x = x_test_batch[:16].to(device)
        xhat, _, _ = vae(x)
    show_grid(x.cpu(), title=f"VAE LATENT={LATENT} inputs (Epochs={GLOBAL_EPOCHS})",
              save_path=os.path.join(GAN_PICS_DIR, f"vae_inputs_latent_{LATENT}_epochs_{GLOBAL_EPOCHS}.png"))
    show_grid(xhat.cpu(), title=f"VAE LATENT={LATENT} reconstructions (Epochs={GLOBAL_EPOCHS})",
              save_path=os.path.join(GAN_PICS_DIR, f"vae_reconstructions_latent_{LATENT}_epochs_{GLOBAL_EPOCHS}.png"))

    # VAE random samples
    with torch.no_grad():
        z = torch.randn(16, LATENT, device=device)
        samples_vae = vae.decode(z).cpu()
    show_grid(samples_vae, title=f"VAE LATENT={LATENT} random samples (Epochs={GLOBAL_EPOCHS})",
              save_path=os.path.join(GAN_PICS_DIR, f"vae_random_samples_latent_{LATENT}_epochs_{GLOBAL_EPOCHS}.png"))

    # VAE Latent interpolation
    with torch.no_grad():
        x, _ = next(iter(test_loader))
        x = x.to(device)[:2]
        mu, logvar = vae.encode(x)
        z1 = mu[0]; z2 = mu[1]
        z_traj = interpolate(z1, z2, steps=16)
        interp_imgs = vae.decode(z_traj).cpu()
    show_grid(interp_imgs, title=f"VAE LATENT={LATENT} latent interpolation (Epochs={GLOBAL_EPOCHS})",
              save_path=os.path.join(GAN_PICS_DIR, f"vae_latent_interpolation_latent_{LATENT}_epochs_{GLOBAL_EPOCHS}.png"))

    # Calculate FID for VAE
    if SCIPY_OK:
        # Need to generate enough VAE samples to match real_feats count
        # We need real_feats from GAN part, but it might not be available if cells run out of order.
        # However, in a script, they run in order.
        # We should re-calculate real_feats here to be safe or assume D is available.
        # D is available from Part 1.
        
        real_feats = get_features(D, test_loader, n_batches=80, use_fake=False) # Recalculate to be safe
        mu_r, sig_r = gaussian_stats(real_feats)

        all_vae_samples = []
        num_samples_needed = len(real_feats)
        num_batches_vae_fid = math.ceil(num_samples_needed / BATCH)

        for _ in range(num_batches_vae_fid):
            z_batch = torch.randn(BATCH, LATENT, device=device)
            all_vae_samples.append(vae.decode(z_batch).cpu())
        all_vae_samples = torch.cat(all_vae_samples, dim=0)[:num_samples_needed]

        fake_feats_vae = []
        for i in range(0, len(all_vae_samples), BATCH):
            batch = all_vae_samples[i:i+BATCH].to(device)
            _, f = D(batch)
            f = F.adaptive_avg_pool2d(f, 1).flatten(1)
            fake_feats_vae.append(f.cpu())
        fake_feats_vae = torch.cat(fake_feats_vae, dim=0).numpy()
        mu_v, sig_v = gaussian_stats(fake_feats_vae)
        fid_vae = frechet_distance(mu_r, sig_r, mu_v, sig_v)
        print(f"Proxy FID (VAE, LATENT={LATENT} vs real): {fid_vae:.2f}")
    else:
        fid_vae = float('nan')

    vae_results.append({'latent_dim': LATENT, 'fid': fid_vae})


"""
---

# Part 3 ‚Äî **Comparison & Proxy FID-like Metric**  *(~30 minutes)*

We will compare samples from the GAN and VAE using a **feature Fr√©chet distance** proxy:
1) Extract features from the **Discriminator** (penultimate conv layer)
2) Fit Gaussians to **real** vs **fake** features
3) Compute **Fr√©chet distance**:  
   $\|\mu_r-\mu_f\|^2 + \mathrm{Tr}(\Sigma_r + \Sigma_f - 2(\Sigma_r \Sigma_f)^{1/2})$




> This is not the official FID (which uses Inception), but behaves similarly for quick lab work.
"""

# FID helpers moved to top

# Single run FID calculation and plotting removed.


"""---

## üìù Final Reflection (to submit)

1. Copy all the generated outputs, don't forget to label them (e.g   Fashion-MNIST, GAN, Z_DIM=128, EPOCH=... )

2. Include image grids:
   - GAN samples (best epoch)
   - VAE reconstructions
   - VAE latent interpolation

3. Include your **proxy FID-like** numbers for GAN and VAE.

4. Answer briefly:
   - What hyperparameters most influenced **GAN stability** in your runs?
   - Evidence of **mode collapse** (if any)? What helped?
   - How did **latent dim** affect VAE reconstructions and samples?
   - One idea to combine benefits of both models (e.g., **VAE-GAN**).
"""

# ===== (TODO) Hyperparameter Tuning & Comparison =====
# Plotting the results
import matplotlib.pyplot as plt
import numpy as np

print("\n--- All experiments completed ---")
print("GAN Results:", gan_results)
print("VAE Results:", vae_results)

# Prepare data for plotting
# We assume the lists Z_DIM_OPTIONS and LATENT_OPTIONS have the same length and correspond to
# roughly "Low", "Medium", and "High" capacity models.
# We will plot a grouped bar chart to compare GAN vs VAE at each level.

n_groups = len(gan_results)
gan_fids = [r['fid'] for r in gan_results]
vae_fids = [r['fid'] for r in vae_results]
z_labels = [r['z_dim'] for r in gan_results]
l_labels = [r['latent_dim'] for r in vae_results]

# Create plot
fig, ax = plt.subplots(figsize=(10, 6))
index = np.arange(n_groups)
bar_width = 0.35
opacity = 0.8

rects1 = ax.bar(index, gan_fids, bar_width,
                alpha=opacity, color='skyblue',
                label='GAN (Z_DIM)')

rects2 = ax.bar(index + bar_width, vae_fids, bar_width,
                alpha=opacity, color='lightcoral',
                label='VAE (LATENT)')

ax.set_xlabel('Model Capacity / Hyperparameter Setting')
ax.set_ylabel('Proxy FID (Lower is better)')
ax.set_title(f'Paired Comparison: GAN vs VAE Performance (Epochs={GLOBAL_EPOCHS})')
ax.set_xticks(index + bar_width / 2)
ax.set_xticklabels([f"Low\n(Z={z}, L={l})" if i==0 else 
                    f"Mid\n(Z={z}, L={l})" if i==1 else 
                    f"High\n(Z={z}, L={l})" 
                    for i, (z, l) in enumerate(zip(z_labels, l_labels))])
ax.legend()

# Add value labels
ax.bar_label(rects1, padding=3, fmt='%.2f')
ax.bar_label(rects2, padding=3, fmt='%.2f')

plt.tight_layout()
# Construct descriptive filename
z_str = "_".join(map(str, z_labels))
l_str = "_".join(map(str, l_labels))
save_path = os.path.join(GAN_PICS_DIR, f"paired_fid_comparison_z_{z_str}_latent_{l_str}_epochs_{GLOBAL_EPOCHS}.png")
plt.savefig(save_path)
print(f"Paired comparison plot saved to {save_path}")
plt.show()
